{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cec3213",
   "metadata": {},
   "source": [
    "## Data Version Control (DVC) exercise - creating a pipeline\n",
    "\n",
    "This exercise will use DVC to capture a data pipeline recreating the steps performed in training a periodic spline model on historic temperature range measurements in [_Lecture 22 (Data Pipelines)_](Lecture22_DataPipelines.ipynb). This exercise will assume you have some familiarity with using Git from a command-line. If you are not familiar with the Git command line interface you can run just the DVC commands without also tracking the changes with Git - in this case you will need [to pass the `--no-scm` option to `dvc init`](https://dvc.org/doc/command-reference/init#initializing-dvc-without-git) and ignore the exercise parts <span style=\"color: red;\"> in red</span>.\n",
    "\n",
    "Running the cell below creates a new `dvc-pipeline-example` directory in the current working directory and changes this to the working directory. This ensures the files we create to construct the example pipeline will be kept isolated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dad8394b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T00:48:44.206725Z",
     "iopub.status.busy": "2024-01-10T00:48:44.206286Z",
     "iopub.status.idle": "2024-01-10T00:48:44.213358Z",
     "shell.execute_reply": "2024-01-10T00:48:44.212754Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.mkdir(\"dvc-pipeline-example\")\n",
    "os.chdir(\"dvc-pipeline-example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be45eb6",
   "metadata": {},
   "source": [
    "To use DVC you will need to have [DVC installed](https://dvc.org/doc/install) and to be able to run the `dvc` command from a [shell](https://en.wikipedia.org/wiki/Shell_(computing)) (command-line interface to the operating system). <span style=\"color: red;\">You will also need to have Git installed and be able to run `git` for the Git parts of the exercises.</span> If you are running the Jupyter notebook server from a Linux or MacOS system (or from a [Windows Subsystem for Linux](https://learn.microsoft.com/en-us/windows/wsl/install) shell on Windows) then you can use the [`%%sh`](https://ipython.readthedocs.io/en/stable/interactive/magics.html#cellmagic-bash) IPython cell magic command to run shell commands directly from cells in the notebook - just write `%%sh` as the first line of the cell followed by one or more lines corresponding to the commands to execute. On Windows, you can launch an Anaconda Prompt terminal and run the commands from there (ensuring `dvc` and `git` are installed in the activated `conda` environment and that you are within the `dvc-pipeline-example` directory)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82815dd6",
   "metadata": {},
   "source": [
    "## Initialising Git and DVC\n",
    "\n",
    "As a first step \n",
    "  1. <span style=\"color: red;\">Create a new empty Git repository in the `dvc-pipeline-example` directory.</span>\n",
    "  2. Initialise a new DVC project in the `dvc-pipeline-example` directory.\n",
    "  3. <span style=\"color: red;\">Commit the files created by DVC to the current Git branch.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f7d0b9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T00:48:44.216370Z",
     "iopub.status.busy": "2024-01-10T00:48:44.216150Z",
     "iopub.status.idle": "2024-01-10T00:48:44.781780Z",
     "shell.execute_reply": "2024-01-10T00:48:44.780907Z"
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hint: Using 'master' as the name for the initial branch. This default branch name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hint: is subject to change. To configure the initial branch name to use in all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hint: of your new repositories, which will suppress this warning, call:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hint: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hint: \tgit config --global init.defaultBranch <name>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hint: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hint: 'development'. The just-created branch can be renamed via this command:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hint: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hint: \tgit branch -m <name>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized empty Git repository in /home/runner/work/course_mlbd/course_mlbd/Lectures/dvc-pipeline-example/.git/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized DVC repository.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can now commit the changes to git.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                                                                     |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|        DVC has enabled anonymous aggregate usage analytics.         |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|     Read the analytics documentation (and how to opt-out) here:     |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|             <https://dvc.org/doc/user-guide/analytics>              |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                                                                     |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's next?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Check out the documentation: <https://dvc.org/doc>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Get help and share ideas: <https://dvc.org/chat>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Star us on GitHub: <https://github.com/iterative/dvc>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master (root-commit) 4a853be] Initialize DVC\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3 files changed, 6 insertions(+)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " create mode 100644 .dvc/.gitignore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " create mode 100644 .dvc/config\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " create mode 100644 .dvcignore\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "git init\n",
    "dvc init\n",
    "git commit -m \"Initialize DVC\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68e9b83",
   "metadata": {},
   "source": [
    "## Importing the remote data file\n",
    "\n",
    "  1. Use the `dvc import-url` command to import the gzipped CSV file `1800.csv.gz` containing the historical climate records for the year 1800 from the `noaa-ghcn-pds` Amazon Web Services (AWS) S3 bucket in to the local project.\n",
    "  2. <span style=\"color: red;\">Add the files generated by DVC on importing the data file to the staging area and commit these changes with Git.</span>\n",
    "  \n",
    "_Hint_: To avoid having provide Amazon Web Service credentials (which are required when using DVC's built in support for accesing AWS S3 storage) you can instead use HTTP to access data on the bucket using a URL of the form `http://{bucket-name}.s3.amazonaws.com/{path-to-file}` where `{bucket-name}` is the name of the (publicly accessible) S3 bucket to access and `{path-to-file}` is the path to the file to access on the bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26f3cca4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T00:48:44.785911Z",
     "iopub.status.busy": "2024-01-10T00:48:44.785659Z",
     "iopub.status.idle": "2024-01-10T00:48:46.527605Z",
     "shell.execute_reply": "2024-01-10T00:48:46.526846Z"
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing 'http://noaa-ghcn-pds.s3.amazonaws.com/csv.gz/1800.csv.gz' -> '1800.csv.gz'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To track the changes with git, run:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tgit add 1800.csv.gz.dvc .gitignore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To enable auto staging, run:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tdvc config core.autostage true\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 0a9f8af] Initial data import\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2 files changed, 13 insertions(+)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " create mode 100644 .gitignore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " create mode 100644 1800.csv.gz.dvc\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "dvc import-url http://noaa-ghcn-pds.s3.amazonaws.com/csv.gz/1800.csv.gz 1800.csv.gz\n",
    "git add .gitignore 1800.csv.gz.dvc\n",
    "git commit -m \"Initial data import\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d58f840",
   "metadata": {},
   "source": [
    "## Writing a Python script to prepare data for model training\n",
    "\n",
    "Create a Python script `prepare.py` in the `dvc-pipeline-example` directory which recreates the preprocess, transform and serve stages of the example pipeline the [_Lecture 22 (Data Pipelines)_ notebook](Lecture22_DataPipelines.ipynb). That is the script should\n",
    "\n",
    "  1. Extract the CSV data from the downloaded `1800.csv.gz` file.\n",
    "  2. Select the records corresponding to temperature (`TMIN` and `TMAX`) measurements for station `ITE00100554`.\n",
    "  3. Transform the extreme temperature measurements to temperature ranges in degrees Celsius.\n",
    "  4. Write out the transformed data as a feature matrix (containing the day of year of the measurement as integers) and targets array (containing the temperature ranges in degrees Celsius) ready to feed in to a scikit-learn regression model.\n",
    "\n",
    "The script should take as arguments the path to the input data file and the path to write the prepared training data to.\n",
    "\n",
    "_Hint 1_: Most of the code you need is already in the [_Lecture 22 (Data Pipelines)_ notebook](Lecture22_DataPipelines.ipynb).  \n",
    "_Hint 2_: The [`numpy.savez`](https://numpy.org/doc/stable/reference/generated/numpy.savez.html) function may be useful for writing NumPy array data to a file.  \n",
    "_Hint 3_: You can use [the `%%writefile` cell magic](https://ipython.readthedocs.io/en/stable/interactive/magics.html#cellmagic-writefile) to create a Python script file from the contents of a cell within the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c859f8db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T00:48:46.531086Z",
     "iopub.status.busy": "2024-01-10T00:48:46.530840Z",
     "iopub.status.idle": "2024-01-10T00:48:46.537971Z",
     "shell.execute_reply": "2024-01-10T00:48:46.537377Z"
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing prepare.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prepare.py\n",
    "\"\"\"Prepare historical temperature records data for model training\"\"\"\n",
    "\n",
    "import argparse\n",
    "import gzip\n",
    "import numpy\n",
    "import pandas\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def preprocess(gzipped_data_path):\n",
    "    with gzip.open(gzipped_data_path, \"rb\") as f:\n",
    "        input_data = pandas.read_csv(\n",
    "            f, usecols=range(4), names=[\"station\", \"date\", \"quantity\", \"value\"]\n",
    "        )\n",
    "    return input_data.query(\n",
    "        'station == \"ITE00100554\" and quantity in [\"TMIN\", \"TMAX\"]'\n",
    "    )\n",
    "\n",
    "\n",
    "def transform(preprocessed_data):\n",
    "    pivotted_data = preprocessed_data.pivot(\n",
    "        index=\"date\", columns=\"quantity\", values=\"value\"\n",
    "    )\n",
    "    pivotted_data.index = pandas.to_datetime(pivotted_data.index, format=\"%Y%m%d\")\n",
    "    return pandas.DataFrame(\n",
    "        {\"temperature_range\": (pivotted_data.TMAX - pivotted_data.TMIN) / 10}\n",
    "    )\n",
    "\n",
    "\n",
    "def prepare_for_sklearn(transformed_data):\n",
    "    return {\n",
    "        \"features\": transformed_data.index.day_of_year.array[:, None],\n",
    "        \"targets\": transformed_data.temperature_range.array\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\"Prepare training data\")\n",
    "    parser.add_argument(\"--input-data-file\", type=Path, required=True)\n",
    "    parser.add_argument(\"--output-file\", type=Path, required=True)\n",
    "    args = parser.parse_args()\n",
    "    preprocessed_data = preprocess(args.input_data_file)\n",
    "    transformed_data = transform(preprocessed_data)\n",
    "    training_data = prepare_for_sklearn(transformed_data)\n",
    "    numpy.savez(args.output_file, **training_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1050b5d8",
   "metadata": {},
   "source": [
    "## Adding data preparation pipeline stage to DVC\n",
    "\n",
    "Add a stage to the DVC data pipeline corresponding to preparing the data for model training using the script you just created. The stage should\n",
    "\n",
    "  * Be named `prepare`.\n",
    "  * Have as dependencies the `prepare.py` script and the `1800.csv.gz` data file.\n",
    "  * Have an output file containing the prepared training data feature matrix and target array.\n",
    "  * Execute the `prepare.py` script using `python` as the pipeline command, passing in the input data file and output file name as arguments.\n",
    "\n",
    "You can add the stage either by directly creating a `dvc.yaml` file or using the `dvc stage add` command. <span style=\"color: red;\">Once you have created the stage add the relevant files for the stage to the Git staging area and create a new commit.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28abf7a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T00:48:46.541294Z",
     "iopub.status.busy": "2024-01-10T00:48:46.540872Z",
     "iopub.status.idle": "2024-01-10T00:48:47.275979Z",
     "shell.execute_reply": "2024-01-10T00:48:47.275256Z"
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage 'prepare' in 'dvc.yaml'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To track the changes with git, run:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tgit add dvc.yaml .gitignore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To enable auto staging, run:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tdvc config core.autostage true\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master e45ea95] Adding data preparation stage of pipeline\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3 files changed, 54 insertions(+)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " create mode 100644 dvc.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " create mode 100644 prepare.py\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "dvc stage add \\\n",
    "  -n prepare \\\n",
    "  -d prepare.py -d 1800.csv.gz \\\n",
    "  -o training_data.npz \\\n",
    "  python prepare.py --input-data-file 1800.csv.gz --output-file training_data.npz\n",
    "git add .gitignore dvc.yaml prepare.py\n",
    "git commit -m \"Adding data preparation stage of pipeline\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674714e7",
   "metadata": {},
   "source": [
    "## Writing a Python script to train scikit-learn regression model\n",
    "\n",
    "Create a Python script `train.py` in the `dvc-example-pipeline` directory which recreates the model training stage of the example pipeline the [_Lecture 22 (Data Pipelines)_ notebook](Lecture22_DataPipelines.ipynb). That is the script should\n",
    "\n",
    "  1. Create scikit-learn pipeline corresponding to [a periodic spline regression model with L2 regularization](https://scikit-learn.org/stable/auto_examples/linear_model/plot_polynomial_interpolation.html#periodic-splines).\n",
    "  2. Fit the model to the training data feature matrix and targets array prepared by the previous pipeline stage.\n",
    "  3. Serialize the trained model to a file.\n",
    "  \n",
    "The number of knots to use in the spline model, degree of the spline polynomial and [L2 regularization term $\\alpha$ coefficient](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) should all be configurable parameters read from an external file (for example in JSON or YAML format) to allow them to be [captured as parameters of the pipeline stage](https://dvc.org/doc/start/data-management/metrics-parameters-plots#defining-stage-parameters). The script should accept as arguments the path to the file containing the prepared training data (output of the previous pipeline stage), the path to the parameters file and the path to output a serialization of the trained model to.\n",
    "\n",
    "_Hint 1_: Most of the code you need is again in the [_Lecture 22 (Data Pipelines)_ notebook](Lecture22_DataPipelines.ipynb).  \n",
    "_Hint 2_: The [`json` module in the Python standard library](https://docs.python.org/3/library/json.html) or [third-party PyYAML package](https://pyyaml.org/wiki/PyYAMLDocumentation) may be useful for reading from the parameter file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "072f34e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T00:48:47.279379Z",
     "iopub.status.busy": "2024-01-10T00:48:47.279134Z",
     "iopub.status.idle": "2024-01-10T00:48:47.285741Z",
     "shell.execute_reply": "2024-01-10T00:48:47.285154Z"
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "\"\"\"Train periodic spline model on prepared data\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import numpy\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from sklearn import pipeline, preprocessing, linear_model\n",
    "\n",
    "\n",
    "def train_model(training_data, num_knots, spline_degree, ridge_alpha):\n",
    "    knots = numpy.linspace(1, 365, num_knots)[:, None]\n",
    "    model = pipeline.make_pipeline(\n",
    "        preprocessing.SplineTransformer(\n",
    "            degree=spline_degree, knots=knots, extrapolation=\"periodic\"\n",
    "        ),\n",
    "        linear_model.Ridge(ridge_alpha)\n",
    "    )\n",
    "    model.fit(training_data[\"features\"], training_data[\"targets\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\"Train model\")\n",
    "    parser.add_argument(\"--training-data-file\", type=Path, required=True)\n",
    "    parser.add_argument(\"--parameters-file\", type=Path, required=True)\n",
    "    parser.add_argument(\"--output-file\", type=Path, required=True)\n",
    "    args = parser.parse_args()\n",
    "    with open(args.parameters_file, \"r\") as f:\n",
    "        parameters = json.load(f)\n",
    "    training_data = numpy.load(args.training_data_file)\n",
    "    model = train_model(training_data, **parameters[\"train_model\"])\n",
    "    with open(args.output_file, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad3ed88",
   "metadata": {},
   "source": [
    "## Adding a model training pipeline stage\n",
    "\n",
    "Add a stage to the DVC data pipeline corresponding to training the model using the script you just created. The stage should\n",
    "\n",
    "  * Be named `train`.\n",
    "  * Have as parameters the number of spline knots, spline polynomial degree and L2 regularization coefficient $\\alpha$.\n",
    "  * Have as dependencies the `train.py` script and training data file outputted by the previous `prepare` stage.\n",
    "  * Have an output file corresponding to the serialized trained model.\n",
    "  * Execute the `train.py` script using `python` as the pipeline command, passing in the training data file, parameters file and output file name as arguments.\n",
    "  \n",
    "You will also need to create the parameter file - for the parameter values we suggest using 7 spline knots, a spline degree of 3 and $\\alpha$ L2 coefficient value of 0.01. <span style=\"color: red;\">Once you have created the stage add the relevant files for the stage to the Git staging area and create a new commit.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa06a508",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T00:48:47.288987Z",
     "iopub.status.busy": "2024-01-10T00:48:47.288565Z",
     "iopub.status.idle": "2024-01-10T00:48:47.292360Z",
     "shell.execute_reply": "2024-01-10T00:48:47.291793Z"
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing params.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile params.json\n",
    "{\n",
    "    \"train_model\": {\"num_knots\": 7, \"spline_degree\": 3, \"ridge_alpha\": 1e-2}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a95e2e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T00:48:47.297086Z",
     "iopub.status.busy": "2024-01-10T00:48:47.296538Z",
     "iopub.status.idle": "2024-01-10T00:48:48.100851Z",
     "shell.execute_reply": "2024-01-10T00:48:48.100017Z"
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage 'train' in 'dvc.yaml'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To track the changes with git, run:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tgit add .gitignore dvc.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To enable auto staging, run:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tdvc config core.autostage true\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master a33a1f4] Adding model training stage of pipeline\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4 files changed, 51 insertions(+)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " create mode 100644 params.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " create mode 100644 train.py\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "dvc stage add \\\n",
    "  -n train \\\n",
    "  -p params.json:train_model.num_knots,train_model.spline_degree,train_model.ridge_alpha \\\n",
    "  -d train.py -d training_data.npz \\\n",
    "  -o model.pkl \\\n",
    "  python train.py --training-data-file training_data.npz --parameters-file params.json --output-file model.pkl\n",
    "git add .gitignore dvc.yaml params.json train.py\n",
    "git commit -m \"Adding model training stage of pipeline\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce66b3d",
   "metadata": {},
   "source": [
    "## Writing a Python script to plot model predictions\n",
    "\n",
    "Create a Python script `plot.py` in the `dvc-example-pipeline` directory which plots the trained model's predictions as in the example pipeline the [_Lecture 22 (Data Pipelines)_ notebook](Lecture22_DataPipelines.ipynb). That is the script should\n",
    "\n",
    "  1. Load the serialized train model and training data from files.\n",
    "  2. Compute the model predictions on the training data input features.\n",
    "  3. Plot the model predictions and training target values (daily temperature range in degrees Celsius) against the corresponding input feature values (day of the year) as separate line plots on the same axes using Matplotib.\n",
    "  4. Save the generated plot figure to a file.\n",
    "\n",
    "The script should accept as arguments the path to the file containing the  training data, the path to the file containing the serialized trained model and the path to output the saved figure to.\n",
    "\n",
    "_Hint 1_: Most of the code you need is again in the [_Lecture 22 (Data Pipelines)_ notebook](Lecture22_DataPipelines.ipynb).  \n",
    "_Hint 2_: The [Matplotlib `Figure.savefig` method](https://matplotlib.org/stable/api/figure_api.html#matplotlib.figure.Figure.savefig) may be useful for saving the plot figure to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c406c57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T00:48:48.104352Z",
     "iopub.status.busy": "2024-01-10T00:48:48.104104Z",
     "iopub.status.idle": "2024-01-10T00:48:48.111460Z",
     "shell.execute_reply": "2024-01-10T00:48:48.110788Z"
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing plot.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile plot.py\n",
    "\"\"\"Plot trained model predictions\"\"\"\n",
    "\n",
    "import argparse\n",
    "import matplotlib.pyplot\n",
    "import numpy\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def plot_data_and_model_predictions(training_data, model, **fig_kwargs):\n",
    "    fig, ax = matplotlib.pyplot.subplots(**fig_kwargs)\n",
    "    ax.plot(\n",
    "        training_data[\"features\"],\n",
    "        training_data[\"targets\"],\n",
    "        model.predict(training_data[\"features\"])\n",
    "    )\n",
    "    ax.set(xlabel=\"Day of year\", ylabel=\"Daily temperature range / $^\\circ C$\")\n",
    "    ax.legend([\"Data\", \"Spline fit\"])\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\"Train model\")\n",
    "    parser.add_argument(\"--training-data-file\", type=Path, required=True)\n",
    "    parser.add_argument(\"--model-file\", type=Path, required=True)\n",
    "    parser.add_argument(\"--output-file\", type=Path, required=True)\n",
    "    args = parser.parse_args()\n",
    "    training_data = numpy.load(args.training_data_file)\n",
    "    with open(args.model_file, \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    fig, ax = plot_data_and_model_predictions(training_data, model, figsize=(8, 3))\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(args.output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2d3540",
   "metadata": {},
   "source": [
    "## Adding a plotting pipeline stage\n",
    "\n",
    "Add a stage to the DVC data pipeline corresponding to plotting the model predictions using the script you just created. The stage should\n",
    "\n",
    "  * Be named `plot`.\n",
    "  * Have as dependencies the `plot.py` script, and training data and serialized model files outputted by the previous stages.\n",
    "  * Have an output file corresponding to the generated figure.\n",
    "  * Execute the `plot.py` script using `python` as the pipeline command, passing in the training data file, trained model file and output file name as arguments.\n",
    "  \n",
    "<span style=\"color: red;\">Once you have created the stage add the relevant files for the stage to the Git staging area and create a new commit.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbd80b2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T00:48:48.114578Z",
     "iopub.status.busy": "2024-01-10T00:48:48.114333Z",
     "iopub.status.idle": "2024-01-10T00:48:48.868156Z",
     "shell.execute_reply": "2024-01-10T00:48:48.867279Z"
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added stage 'plot' in 'dvc.yaml'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To track the changes with git, run:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tgit add .gitignore dvc.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To enable auto staging, run:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tdvc config core.autostage true\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 93d3c6e] Adding prediction plotting stage of pipeline\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3 files changed, 43 insertions(+)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " create mode 100644 plot.py\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "dvc stage add \\\n",
    "  -n plot \\\n",
    "  -d plot.py -d training_data.npz -d model.pkl \\\n",
    "  -o predictions.pdf \\\n",
    "  python plot.py --training-data-file training_data.npz --model-file model.pkl --output-file predictions.pdf\n",
    "git add .gitignore dvc.yaml plot.py\n",
    "git commit -m \"Adding prediction plotting stage of pipeline\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e403b97",
   "metadata": {},
   "source": [
    "## Visualizing and running the pipeline\n",
    "\n",
    "Visualize the DVC data pipeline you have created as a directed acyclic graph and test running the pipeline.\n",
    "\n",
    "\n",
    "_Hint_: The [DVC Data Pipelines documentation](https://dvc.org/doc/start/data-management/data-pipelines) explains the commands you need here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8686916d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T00:48:48.871530Z",
     "iopub.status.busy": "2024-01-10T00:48:48.871266Z",
     "iopub.status.idle": "2024-01-10T00:48:53.463742Z",
     "shell.execute_reply": "2024-01-10T00:48:53.462876Z"
    },
    "tags": [
     "solution"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    +-----------------+  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    | 1800.csv.gz.dvc |  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    +-----------------+  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              *          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              *          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              *          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        +---------+      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        | prepare |      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        +---------+      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         *        *      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       **          *     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      *             **   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+             *  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| train |           **   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+          *     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         *        *      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          **    **       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            *  *         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          +------+       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          | plot |       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          +------+       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'1800.csv.gz.dvc' didn't change, skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running stage 'prepare':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> python prepare.py --input-data-file 1800.csv.gz --output-file training_data.npz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating lock file 'dvc.lock'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating lock file 'dvc.lock'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running stage 'train':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> python train.py --training-data-file training_data.npz --parameters-file params.json --output-file model.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating lock file 'dvc.lock'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running stage 'plot':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> python plot.py --training-data-file training_data.npz --model-file model.pkl --output-file predictions.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating lock file 'dvc.lock'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To track the changes with git, run:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tgit add dvc.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To enable auto staging, run:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tdvc config core.autostage true\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use `dvc push` to send your updates to remote storage.\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "dvc dag\n",
    "dvc repro"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
